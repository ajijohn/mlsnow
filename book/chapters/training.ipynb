{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import expanduser\n",
    "sys.path.append(expanduser(\"~/robosat.pink/\"))\n",
    "\n",
    "from robosat_pink.datasets import *\n",
    "from robosat_pink.tiles import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from robosat_pink.models import albunet\n",
    "from robosat_pink.tools.train import train, validate\n",
    "from robosat_pink.losses.lovasz import Lovasz\n",
    "import robosat_pink\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from skimage import exposure\n",
    "\n",
    "from numpy.random import choice\n",
    "from math import floor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from os import environ\n",
    "environ['CURL_CA_BUNDLE']='/etc/ssl/certs/ca-certificates.crt'\n",
    "environ['AWS_DEFAULT_PROFILE'] = 'esip'\n",
    "\n",
    "from imp import reload\n",
    "reload(robosat_pink.losses.lovasz)\n",
    "reload(robosat_pink.tools.train)\n",
    "from robosat_pink.tools.train import train, validate\n",
    "from robosat_pink.losses.lovasz import Lovasz\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "CHECKPOINT = \"s3://planet-snowcover-models/checkpoint-190319-20:47:57\"\n",
    "S3_CHECKPOINT = False\n",
    "if CHECKPOINT.startswith(\"s3://\"):\n",
    "    S3_CHECKPOINT = True\n",
    "    # load from s3 \n",
    "    CHECKPOINT = CHECKPOINT[5:]\n",
    "    sess = boto3.Session(profile_name=environ['AWS_DEFAULT_PROFILE'])\n",
    "    fs = s3fs.S3FileSystem(session=sess)\n",
    "    s3ckpt = s3fs.S3File(fs, CHECKPOINT, 'rb')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"s3://planet-snowcover-imagery/20180601_181450_0f32_3B_AnalyticMS_SR_clip_tiled\"\n",
    "DATA_DIR = \"s3://planet-snowcover-imagery/20180601_181448_0f32_3B_AnalyticMS_SR_clip_tiled\"\n",
    "MASK_DIR = \"s3://planet-snowcover-snow/ASO_3M_SD_USCASJ_20180601_tiles_02\"\n",
    "all_tiles = SlippyMapTilesConcatenation(path = DATA_DIR, \n",
    "                                        target = MASK_DIR, \n",
    "                                        aws_profile = 'esip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(all_tiles.tiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    #A.ToFloat(p = 1),\n",
    "    # A.RandomRotate90(p = 0.5),\n",
    "    #A.RandomRotate90(p = 0.5),\n",
    "    #A.RandomRotate90(p = 0.5), #these do something bad to the bands\n",
    "#    A.Normalize(mean = mean, std = std, max_pixel_value = 1),\n",
    "    A.HorizontalFlip(p = 0.5),\n",
    "    A.VerticalFlip(p = 0.5),\n",
    "#    A.ToFloat(p = 1, max_value = np.finfo(np.float64).max)\n",
    "])\n",
    "\n",
    "train_tiles = SlippyMapTilesConcatenation(path = DATA_DIR, \n",
    "                                          target = MASK_DIR, \n",
    "                                          tiles = train_ids,\n",
    "                                          aws_profile = 'esip',\n",
    "                                          joint_transform = transform)\n",
    "valid_tiles = SlippyMapTilesConcatenation(path = DATA_DIR, \n",
    "                                          target = MASK_DIR, \n",
    "                                          tiles = test_ids, \n",
    "                                          aws_profile = 'esip', \n",
    "                                          joint_transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = albunet.Albunet(num_classes = 1, num_channels = 4)\n",
    "device = torch.device('cuda')\n",
    "net = torch.nn.DataParallel(net)\n",
    "\n",
    "criterion = Lovasz().to(device)\n",
    "optimizer = Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "if CHECKPOINT is not None:\n",
    "    def map_location(storage, _):\n",
    "        return storage.cuda() if torch.cuda.is_available() else storage.cpu()\n",
    "    try: \n",
    "        if S3_CHECKPOINT:\n",
    "            with s3fs.S3File(fs, CHECKPOINT, 'rb') as C:\n",
    "                state = torch.load(io.BytesIO(C.read()))\n",
    "        else: \n",
    "            state = torch.load(io.BytesIO(C.read()))\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "        net.load_state_dict(state['state_dict'])\n",
    "        net.to(device)\n",
    "    except FileNotFoundError as f:\n",
    "        print(\"{} checkpoint not found.\".format(CHECKPOINT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_tiles,\n",
    "                          batch_size = 8,\n",
    "                          shuffle  = True,\n",
    "                          drop_last=True, \n",
    "                          num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(valid_tiles,\n",
    "                          batch_size = 8,\n",
    "                          shuffle  = True,\n",
    "                          drop_last=True, \n",
    "                          num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    print(\"epoch {}\".format(epoch))\n",
    "    train_hist = train(train_loader, 1, device, net, optimizer, criterion)\n",
    "    print(train_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look the run stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session(profile_name=environ['AWS_DEFAULT_PROFILE'])\n",
    "fs = s3fs.S3FileSystem(session=sess)\n",
    "f = s3fs.S3File(fs, 'planet-snowcover-models/' + fname, 'wb', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict' : net.state_dict(), 'optimizer': optimizer.state_dict()}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c446eef832ec964573dc49f36fd16bdbed40cbfbefbf557bc2dc78d9e7968689"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
